{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGCwhCOx_9BR"
      },
      "source": [
        "<h1><center>Sistemas Distribuídos – 2023/1\n",
        "\n",
        "Prof. Rodolfo da Silva Villaça – rodolfo.villaca@ufes.br \\\\\n",
        "Monitor: Eduardo M. Moraes Sarmento – eduardo.sarmento@ufes.br\n",
        "\n",
        "Laboratório II – Aprendizado Federado usando a biblioteca Flower</h1></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfbB0PxwAq-q"
      },
      "source": [
        "<u>Objetivo</u>:\\\n",
        "Experimentar o treinamento de modelos de aprendizado de máquina por meio do *framework* de aprendizado *flwr*, disponível na biblioteca *flower* Comparar os resultados atingidos pelo modelo treinado de maneira local e federada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh8VLSKIFsFV"
      },
      "source": [
        "Este roteiro de laboratório foi testado usando o [Google Colab](https://colab.research.google.com/), ambiente de desenvolvimento colaborativo Python disponibilizado pelo Google como um serviço em nuvem.\\\n",
        "\\\n",
        "Para a execução do código deste roteiro são necessárias as bibliotecas *tensorflow* (versão 2.12.0), *numpy* (versão 1.22.4), *ray* (versão 2.2.0), *matplotlib* (versão 3.7.1) e *flower* (versão 1.3.0). As quatro primeiras já vem instaladas no **Google Colab**,. ntão, caso esteja executando por ele, não é necessária sua instalação. A biblioteca flower não vem instalada,  então precisamos instalá-la, e também o seu módulo de simulação, para podermos simular o treinamento federado no *notebook*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzNS6Vh14t8O"
      },
      "source": [
        "A célula seguinte executa a instalação de todas as bibliotecas necessárias. Caso esteja fora do Colab, é preciso descomentar as linhas de 3 a 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZR5l58I7JhXc",
        "outputId": "385d49c1-8a68-404a-d4f7-2b0cdfb9915f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flwr==1.3.0\n",
            "  Using cached flwr-1.3.0-py3-none-any.whl (139 kB)\n",
            "Requirement already satisfied: grpcio!=1.52.0,<2.0.0,>=1.43.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from flwr==1.3.0) (1.49.1)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from flwr==1.3.0) (0.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from flwr==1.3.0) (1.22.4)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.19.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from flwr==1.3.0) (3.20.3)\n",
            "Requirement already satisfied: six>=1.5.2 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from grpcio!=1.52.0,<2.0.0,>=1.43.0->flwr==1.3.0) (1.16.0)\n",
            "Installing collected packages: flwr\n",
            "  Attempting uninstall: flwr\n",
            "    Found existing installation: flwr 0.16.0\n",
            "    Uninstalling flwr-0.16.0:\n",
            "      Successfully uninstalled flwr-0.16.0\n",
            "Successfully installed flwr-1.3.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: flwr[simulation] in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (1.3.0)\n",
            "Requirement already satisfied: grpcio!=1.52.0,<2.0.0,>=1.43.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from flwr[simulation]) (1.49.1)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from flwr[simulation]) (0.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from flwr[simulation]) (1.22.4)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.19.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from flwr[simulation]) (3.20.3)\n",
            "Collecting flwr[simulation]\n",
            "  Using cached flwr-1.3.0-py3-none-any.whl (139 kB)\n",
            "  Using cached flwr-1.2.0-py3-none-any.whl (133 kB)\n",
            "  Using cached flwr-1.1.0-py3-none-any.whl (121 kB)\n",
            "  Using cached flwr-1.0.0-py3-none-any.whl (90 kB)\n",
            "  Using cached flwr-0.19.0-py3-none-any.whl (106 kB)\n",
            "  Using cached flwr-0.18.0-py3-none-any.whl (106 kB)\n",
            "Requirement already satisfied: google<3.0.0,>=2.0.3 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from flwr[simulation]) (2.0.3)\n",
            "Collecting grpcio<=1.43.0,>=1.27.2\n",
            "  Using cached grpcio-1.43.0.tar.gz (21.5 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting flwr[simulation]\n",
            "  Using cached flwr-0.17.0-py3-none-any.whl (229 kB)\n",
            "  Using cached flwr-0.16.0-py3-none-any.whl (216 kB)\n",
            "\u001b[33mWARNING: flwr 0.16.0 does not provide the extra 'simulation'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: beautifulsoup4 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from google<3.0.0,>=2.0.3->flwr[simulation]) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5.2 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from grpcio!=1.52.0,<2.0.0,>=1.43.0->flwr[simulation]) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from beautifulsoup4->google<3.0.0,>=2.0.3->flwr[simulation]) (2.4.1)\n",
            "Installing collected packages: flwr\n",
            "  Attempting uninstall: flwr\n",
            "    Found existing installation: flwr 1.3.0\n",
            "    Uninstalling flwr-1.3.0:\n",
            "      Successfully uninstalled flwr-1.3.0\n",
            "Successfully installed flwr-0.16.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement ray==2.2.0 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for ray==2.2.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: tensorflow==2.12.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (0.4.8)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (67.6.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (2.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (1.49.1)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorflow==2.12.0) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.28.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.2.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.3.1)\n",
            "Collecting numpy<1.24,>=1.22\n",
            "  Using cached numpy-1.23.5-cp311-cp311-macosx_10_9_x86_64.whl (18.1 MB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "Successfully installed numpy-1.23.5\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting numpy==1.22.4\n",
            "  Using cached numpy-1.22.4-cp311-cp311-macosx_13_0_x86_64.whl\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ml-dtypes 0.1.0 requires numpy>=1.23.3; python_version > \"3.10\", but you have numpy 1.22.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.22.4\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: matplotlib==3.7.1 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from matplotlib==3.7.1) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from matplotlib==3.7.1) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from matplotlib==3.7.1) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from matplotlib==3.7.1) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from matplotlib==3.7.1) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from matplotlib==3.7.1) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from matplotlib==3.7.1) (9.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from matplotlib==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from matplotlib==3.7.1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /Users/henrique/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib==3.7.1) (1.16.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install flwr==1.3.0\n",
        "%pip install -U flwr[\"simulation\"]\n",
        "%pip install ray==2.2.0\n",
        "%pip install tensorflow==2.12.0\n",
        "%pip install numpy==1.22.4\n",
        "%pip install matplotlib==3.7.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt4gBsQ7VYUA"
      },
      "source": [
        "###Célula de Importação "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wD8STHmJcZZ"
      },
      "source": [
        "Importamos a biblioteca *os*, que lida com o sistema operacional. Utilizamos ela para configurar a *flag* \"TF_CPP_MIN_LOG_LEVEL\" com o valor 3, isto faz com que os logs do *tensorflow* sejam menos verbosos durante o treinamento.\\\n",
        "Depois, importamos as demais bibliotecas: \n",
        "\n",
        "1.   *flower*, para efetuar o aprendizado federado;\n",
        "2.   *tensorflow*, para definir uma arquitetura de rede neural, incluindo todas as camadas e otimizador que usaremos;\n",
        "3.   *numpy*, biblioteca de maniupulação eficiente de vetores numéricos;\n",
        "4.   *ray*, biblioteca utilizada pelo *flower* para instanciar a simulação do aprendizado federado.\n",
        "5.   *matplotlib*, biblioteca para plotar gráficos.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3dwfd_R6JSkE"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Unable to convert function return value to a Python type! The signature was\n\t() -> handle",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mTF_CPP_MIN_LOG_LEVEL\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m3\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mflwr\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mfl\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Conv2D, MaxPool2D,Flatten,Dense\n",
            "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages/tensorflow/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages/tensorflow/python/__init__.py:42\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n\u001b[1;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[1;32m     44\u001b[0m \u001b[39m# from tensorflow.python import keras\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages/tensorflow/python/data/__init__.py:21\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"`tf.data.Dataset` API for input pipelines.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m experimental\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m AUTOTUNE\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n",
            "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages/tensorflow/python/data/experimental/__init__.py:97\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Experimental API for building input pipelines.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39mThis module contains experimental `Dataset` sources and transformations that can\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39m@@UNKNOWN_CARDINALITY\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m \u001b[39mimport\u001b[39;00m service\n\u001b[1;32m     98\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatching\u001b[39;00m \u001b[39mimport\u001b[39;00m dense_to_ragged_batch\n\u001b[1;32m     99\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatching\u001b[39;00m \u001b[39mimport\u001b[39;00m dense_to_sparse_batch\n",
            "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages/tensorflow/python/data/experimental/service/__init__.py:419\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"API for using the tf.data service.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39mThis module contains:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39m  job of ParameterServerStrategy).\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_service_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[1;32m    420\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_service_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m from_dataset_id\n\u001b[1;32m    421\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_service_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m register_dataset\n",
            "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages/tensorflow/python/data/experimental/ops/data_service_ops.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m data_service_pb2\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m tf2\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m compression_ops\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mservice\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_server_lib\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mservice\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_utils\n",
            "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages/tensorflow/python/data/experimental/ops/compression_ops.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m structure\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m gen_experimental_dataset_ops \u001b[39mas\u001b[39;00m ged_ops\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompress\u001b[39m(element):\n",
            "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages/tensorflow/python/data/util/structure.py:22\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwrapt\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m nest\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m composite_tensor\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m ops\n",
            "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages/tensorflow/python/data/util/nest.py:34\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m\"\"\"## Functions for working with arbitrarily nested sequences of elements.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[39mNOTE(mrry): This fork of the `tensorflow.python.util.nest` module\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m   arrays.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m sparse_tensor \u001b[39mas\u001b[39;00m _sparse_tensor\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_utils\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m nest\n",
            "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages/tensorflow/python/framework/sparse_tensor.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m tf2\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m composite_tensor\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m constant_op\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m dtypes\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m ops\n",
            "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m types_pb2\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m execute\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m dtypes\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m op_callbacks\n",
            "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tfe\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m core\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m dtypes\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m ops\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape\n",
            "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lab2-aprendizado-federado-bA2TOZBT-py3.11/lib/python3.11/site-packages/tensorflow/python/framework/dtypes.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunction\u001b[39;00m \u001b[39mimport\u001b[39;00m trace_type\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdocs\u001b[39;00m \u001b[39mimport\u001b[39;00m doc_controls\n\u001b[0;32m---> 37\u001b[0m _np_bfloat16 \u001b[39m=\u001b[39m _pywrap_bfloat16\u001b[39m.\u001b[39;49mTF_bfloat16_type()\n\u001b[1;32m     38\u001b[0m _np_float8_e4m3fn \u001b[39m=\u001b[39m _pywrap_float8\u001b[39m.\u001b[39mTF_float8_e4m3fn_type()\n\u001b[1;32m     39\u001b[0m _np_float8_e5m2 \u001b[39m=\u001b[39m _pywrap_float8\u001b[39m.\u001b[39mTF_float8_e5m2_type()\n",
            "\u001b[0;31mTypeError\u001b[0m: Unable to convert function return value to a Python type! The signature was\n\t() -> handle"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Make TensorFlow logs less verbose\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "import flwr as fl\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D,Flatten,Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import ray\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMrrg_-wTSSg"
      },
      "source": [
        "#### Importação e Pré-rocessamento dos Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpgFLN0YGu2P"
      },
      "source": [
        "Neste laboratório usaremos o *dataset* MNIST, muito usado como referência na literatura. Este *dataset* é composto por imagens monocromáticas de 28 por 28 píxels (*28x28*), representando dígitos de 0 a 9 escritos a mão e anotadas com o valor do dígito. Ele contém dois *subdatasets*: o de treino e o de teste. O *subdataset* de treino é formado por 60 mil imagens, enquanto o *subdataset* de treino contém 10 mil imagens.\n",
        "\n",
        "O *tensorflow* já nos provê esse *dataset*, separando os *subdatasets* de treino entre: atributos alvo, tambem chamados de classes ou *targets* (*y_train e y_test*), e atributos não alvo (*x_train* e *x_test*), ou *features*. Sendo assim, para carregar este *dataset* basta instanciar (atribuição da variavel *mnist*) e carregar (método *load_data*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Vh4cJYFwJuWl"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mnist \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mmnist\n\u001b[1;32m      2\u001b[0m (x_train, y_train), (x_test, y_test) \u001b[39m=\u001b[39m mnist\u001b[39m.\u001b[39mload_data()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bhElqYx8io1"
      },
      "source": [
        "Plotamos, como exemplo, os 10 primeiros dígitos do *dataset* de treino com suas classes. Por meio desse *plot* conseguiremos visualizar bem o tipo de dado que o *dataset* descreve.\n",
        "\n",
        " Vemos que as classes são o valor numérico que o digito representa e as imagens são dígitos escritos a mão, em preto e branco."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "YqbYR3v-7DyU",
        "outputId": "5613c6e1-4faa-4b79-efda-7e52bf1d3728"
      },
      "outputs": [],
      "source": [
        "num = 10\n",
        "images = x_train[:num]\n",
        "labels = y_train[:num]\n",
        "num_row = 2\n",
        "num_col = 5\n",
        "# plot images\n",
        "fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
        "for i in range(num):\n",
        "    ax = axes[i//num_col, i%num_col]\n",
        "    ax.imshow(images[i], cmap='gray')\n",
        "    ax.set_title('Label: {}'.format(labels[i]))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__UjhTmBJBaY"
      },
      "source": [
        "**Pré-Processamento dos Dados**\n",
        "\n",
        " Primeiro trabalhamos com os dados dos atributos não alvo (*features*), checando o formato dos *datasets* nas variáveis *x_train e x_test*. emos que eles têm os formatos (60000, 28, 28) e (10000, 28, 28) respectivamente.\n",
        "\n",
        " O primeiro número indica a quantidade de imagens contidas em cada *dataset*: 60000 para o treino e 10000 para o teste. Os outros dois números são a quantidade de pixeis de cada imagem, 28 píxeis verticais e 28 píxeis horizontais.\n",
        "\n",
        "Sendo assim, cada imagem é representada como matrizes de números (28 x 28). Já que as imagens são monocromáticas, cada elemento da matriz significa a luminosidade doassociad àquela posição do *pixel*,em valores que vão de 0 a 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU2vN7wXJpMR",
        "outputId": "68ea74bb-cd29-4f00-a667-a751b839c62a"
      },
      "outputs": [],
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2-4DkEMOfzR"
      },
      "source": [
        "Para usar os *datasets* no treinamento e teste de modelos de aprendizado de máquina é necessario, inicialmente, pré-processar os dados.\n",
        "\n",
        "Primeiro fazemos o *reshape*, incluindo uma nova dimensão que indica que as imagens são monocromaticas (*reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)*). \n",
        "\n",
        "Depois normalizamos os dados dos atributos não alvo, dividindo os valores de luminosidade dos pixeis pelo maior valor possivel, *255*, com isso fazendo com que estes valores fiquem na faixa entre 0 e 1. \n",
        "\n",
        "Com essas duas ações encerramos  pré-processamento dos atributos não alvo (*features*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HW1ZnGTP7BY3"
      },
      "outputs": [],
      "source": [
        "x_train=x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
        "x_train=x_train / 255.0\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
        "x_test=x_test/255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI_Yh2zuQTTx"
      },
      "source": [
        "Para os atributos alvo (*target*) é necessário fazer o *one hot enconding* das classes. Esta operação consistem em transformar as classes adicionando novas colunas, que indicam a presença ou não da classe. Isto é, como neste *dataset* as classes são os números de 0 a 9, adicionamos 9 novas colunas, numeradas de 0 a 9. Para cada amostra, cada uma destas novas colunas terá os valores 0 ou 1, onde 0 indica   que a amostra não é daquela classe e 1 indica que a amostra é daquela classe. \n",
        "\n",
        "Printamos a primeira amostra do conjunto de treinamento , antes e depois da aplicação da operação, para vermos seu efeito sobre a amostra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT-Dx92e4_IG",
        "outputId": "d16bfec0-31eb-4f79-89f7-6ae7f2655910"
      },
      "outputs": [],
      "source": [
        "print(y_train[0])\n",
        "y_train = tf.one_hot(y_train.astype(np.int32), depth=10)\n",
        "y_test = tf.one_hot(y_test.astype(np.int32), depth=10)\n",
        "print(y_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVboMW3UTdOp"
      },
      "source": [
        "#### Treinamento Local"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgSqVuHrQcla"
      },
      "source": [
        "A técnica de aprendizado federado foi desenvolvida inicialmente para o treinamento de redes neurais, por isso precisamos definir uma rede neural para podermos usá-la localmente e,  futuramente, comparar os  resultados do  modelos gerados localmente e federado.\n",
        "\n",
        "Esta próxima célula define uma função que monta uma rede neural convolucional simples, que usaremos na etapa de treinamento. A rede neural recebe como atributos: o formato dos dados de entrada (*input_shape*) e o numero de classes do problema (*num_classes*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WcxuXm3AK-n"
      },
      "outputs": [],
      "source": [
        "def define_model(input_shape,num_classes):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n",
        "  model.add(MaxPool2D((2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
        "  model.add(Dense(num_classes, activation='softmax'))\n",
        "  # compile model\n",
        "  opt = SGD(learning_rate=0.01, momentum=0.9)\n",
        "  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xPW2kMNcRtzX"
      },
      "source": [
        "Conforme explicado anteriormente,  após o pré-processamento,  cada imagem do *dataset* terá o seguinte formato: 28 pixeis verticais, 28 pixeis horizontais e 1 canal de cor. Desta forma o formato de entrada dos dados é (28, 28, 1). \n",
        "\n",
        "Temos digitos de 0 a 9 em nosso *dataset*, ou seja, 10 possiveis classes. Com isso instanciamos uma rede neural usando a função *define_model* na variavel model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GycOgcz7JFs"
      },
      "outputs": [],
      "source": [
        "input_shape = (28, 28, 1)\n",
        "num_classes = 10\n",
        "\n",
        "model = define_model(input_shape,num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0VpTq8oShd4"
      },
      "source": [
        "Inicialmente treinaremos a rede neural localmente para termos um *baseline* de comparação. A rede neural  é treinada por 5 épocas, usando 10% do *dataset* de treino para validação durante o treinamento. \n",
        "\n",
        "Usamos 64 como *batch_size*, isso significa que  expomos a rede neural a 64 amostras (lote, ou *batch*) antes de atualizarmos o valor da função de perda. Esse processo  é repetido até que todas as amostras tenham sido expostas à rede neural, terminando uma época de treinamento. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjWO0dJa7VL-",
        "outputId": "4fc3cb20-ff4d-4682-9619-aa78af018384"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "epochs = 5\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_split=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WftT7AFCSud_"
      },
      "source": [
        "Como conclusão, com o conjunto de teste usado neste laboratório, e usando  o nosso modelo,  obtivemos uma acurácia de ~98,39%, que é um resultado aceitável com relação a este *dataset*, comparando-se com os resultados de referência encontrados na literatura. Ver [referência](https://paperswithcode.com/sota/image-classification-on-mnist)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXIiIk8d7eGS",
        "outputId": "7b78060d-98fa-491c-ebf9-f8644230ff0a"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR1XwzYWTiOa"
      },
      "source": [
        "#### Treinamento Federado"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "b3dPiK8tVRn1"
      },
      "source": [
        "Agora começaremos o treinamento federado desta mesma rede neural usando a biblioteca *flower*. Esta biblioteca implementa a arquitetura de aprendizado federado, onde no treinamento temos dois agentes: o servidor e os clientes. \n",
        " \n",
        "* Os clientes (treinadores) têm como tarefa treinar seus modelos usando seus dados locais e, após o treinamento, enviar os pesos encontrados para o servidor de agregação. Quando receberem os pesos agregados pelo servidor, os clientes atualizam seus modelos locais com estes pesos e recomeçam o treinamento, assim iniciando uma nova rodada (época).\n",
        "* O servidor de agregação, por sua vez, recebe estes pesos e usa algum algoritmo para agregar os diferentes pesos dos modelos gerados pelos clientes treinadores. Em seguida o servidor deve enviar de volta os pesos agregados para os clientes. \n",
        "\n",
        "Ao final desse processo,  os  pesos agregados representam o modelo global que está sendo treinado por esse conjunto de treinadores.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTwlQHH6W5Fa"
      },
      "source": [
        "Sabendo disso, para o uso da biblioteca *flower* temos, como primeiro passo, que implementar o código dos clientes. Para isso definimos uma classe cliente que herda da super classe *NumPyClient* provida pela biblioteca *numpy*. Para seu uso temos que implementar obrigatoriamente 3 métodos em nosso cliente:\n",
        "\n",
        "\n",
        "1.  *get_parameters(self, config)*: este método recebe um dicionário de configuração e é chamado pelo servidor como um procedimento remoto. Ele retorna os pesos do modelo do cliente na rodada atual.\n",
        "2.  *fit(self, parameters, config)*: por este método, chamado via RPC pelo servidor, o cliente recebe os pesos do modelo global e um dicionário de configuração. Ele realiza o treinamento do modelo, primeiro setando os pesos do modelo com os recebidos pelo cliente do servidor e depois treinando um novo modelo com esses novos pesos iniciais. Ela retorna: os pesos encontrados após essa rodada de treinamento, o tamanho do conjunto de treinamento e um dicionário de métricas de avaliação do modelo. O dicionário pode ser retornado vazio.\n",
        "3.  *evaluate(self, parameters, config)*: por este método, chamado remotamente pelo servidor, o cliente recebe os pesos do modelo global e um dicionário de configuração. Ele avalia localmente o modelo treinado e retorna o valor da função de perda encontrado para este cliente, o tamanho do conjunto de teste e um dicionário de métricas de avaliação do modelo. Neste caso, retornaremos a acurácia.\n",
        "\n",
        "Também definimos um método opcional *\\_\\_init\\_\\_(self, model, x_train, y_train, x_test, y_test)* que é o de inicialização do objeto, recebendo o modelo pré-instanciado e os conjuntos de teste e treino.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9mL7lqLWQTG"
      },
      "outputs": [],
      "source": [
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, model, x_train, y_train, x_test, y_test) -> None:\n",
        "        self.model = model\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.x_test = x_test\n",
        "        self.y_test = y_test\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return self.model.get_weights()\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        self.model.set_weights(parameters)\n",
        "        self.model.fit(self.x_train, self.y_train, epochs=1, verbose=2)\n",
        "        return self.model.get_weights(), len(self.x_train), {}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        self.model.set_weights(parameters)\n",
        "        loss, acc = self.model.evaluate(self.x_test, self.y_test, verbose=2)\n",
        "        return loss, len(self.x_test), {\"accuracy\": acc}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkXB2eVJbeQk"
      },
      "source": [
        "Como estamos usando um *notebook*, não podemos instanciar clientes e servidores do *flower* diretamente. Por isso a biblioteca prove uma alternativa que é o módulo de simulação. Ele simula o aprendizado federado sem o uso de conexões de rede.\\\n",
        "\\\n",
        "Para o seu uso temos que definir uma função que instancias os clientes com a assinatura *fn(str) -> fl.client.Client*. Esta função recebe uma *string* que é um identificador único do cliente usado pela simulação e nos retorna o cliente instanciado. Internamente ela particiona o *dataset*, amostrando, sem reposição, um número aleatório de imagens, compondo um conjunto de dados que representa os dados locais daquele cliente. \n",
        "\n",
        "A seguir faremos os mesmos passos anteriores de pré-processamento e instanciamento da rede neural. Finalmente instanciaremos um cliente *flower* e retornamos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHd05AShiI46"
      },
      "outputs": [],
      "source": [
        "def client_fn_random(cid: str) -> fl.client.Client:\n",
        "    input_shape = (28, 28, 1)\n",
        "    num_classes = 10\n",
        "    num_clients = 10\n",
        "    partition_size = 500\n",
        "    \n",
        "\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    #sample_size_train = int(cid) * partition_size\n",
        "    #sample_size_test = int(cid) * partition_size\n",
        "    sample_size_train = int((1/num_clients)*len(x_train))\n",
        "    sample_size_test = int((1/num_clients)*len(x_test))\n",
        "    idx_train = np.random.choice(np.arange(len(x_train)), sample_size_train, replace=False)\n",
        "    x_train = x_train[idx_train]/255.0\n",
        "    y_train = y_train[idx_train]\n",
        "    y_train = tf.one_hot(y_train.astype(np.int32), depth=10)\n",
        "    idx_test = np.random.choice(np.arange(len(x_test)), sample_size_test, replace=False)\n",
        "    x_test = x_test[idx_test]/255.0\n",
        "    y_test = y_test[idx_test]\n",
        "    y_test = tf.one_hot(y_test.astype(np.int32), depth=10)\n",
        "    model = define_model(input_shape,num_classes)\n",
        "    # Create and return client\n",
        "    return FlowerClient(model, x_train, y_train, x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txp65BZCdFkD"
      },
      "source": [
        "Para a avaliação do modelo precisamos de uma função de agregação de métricas. Isto é necessário pois, a priori, a simulação não sabe que tipo de métrica será usada para avaliar o modelo. Por isso o programador tem que criar uma função que lide com a agregação dos valores de métricas que ele esta usando para avaliar seu modelo. Desta maneira esta função é usada para que a simulação consiga retornar a evolução dos valores das métricas em cada *round* usando um objeto do tipo *history*.\n",
        "\n",
        "\n",
        "Aqui definimos a função de agregação como a média ponderada da acurácia pelo tamanho do conjunto de dados de cada cliente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZZ0onPuExOM"
      },
      "outputs": [],
      "source": [
        "def weighted_average(metrics):\n",
        "    # Multiply accuracy of each client by number of examples used\n",
        "    acc = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
        "    examples = [num_examples for num_examples, _ in metrics]\n",
        "\n",
        "    # Aggregate and return custom metric (weighted average)\n",
        "    results = {\"accuracy\": sum(acc) / sum(examples)}\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VQnCBXtd0fr"
      },
      "source": [
        "Precisamos agora definir o número de clientes que participação do aprendizado federado, 10 clientes neste exemplo, e uma estratégia de agregação dos pesos a ser usada pelo servidor.\n",
        "\n",
        "Esta estratégia é um objeto do tipo *strategy* da biblioteca *flower*. Além de definir o algoritmo de agregação dos pesos, no caso usamos o *Federated Average (FedAvg)*, também é preciso configurar o comportamento do servidor durante o processo de treinamento. Com essa biblioteca nós configuramos o servidor para:\n",
        "\n",
        "1.   Escolher aleatoriamente 90% dos clientes (9 clientes neste exemplo) para o treinamento (*fraction_fit*);\n",
        "2.   Usar todos os clientes para a avaliação do modelo (*fraction_evaluate*);\n",
        "3.   Nunca usar menos que 9 clientes para o treinamento (*min_fit_clients*);\n",
        "4.   Nunca usar menos que 9 clientes para a avaliação do modelo (*min_evaluate_clients*);\n",
        "5.   Esperar que tenha pelo menos 9 clientes prontos antes de começar o treinamento(*min_available_clients*);\n",
        "6.   Usar a função *weighted_average*, definida anteriormente, como função de agregação de métricas\n",
        "\n",
        "Finalmente iniciamos a simulação usando a função *fl.simulation.start_simulation*. Passamos para essa função a função de instanciamento de clientes (*client_fn_random*), o número de clientes (*num_clients*), um dicionário de configuração, que diz para o servidor quantos *rounds* de treinamento queremos (*fl.server.ServerConfig(num_rounds=5)*) e a estratégia de treinamento (*strategy*). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRvT4OOdXADa",
        "outputId": "bb3b750a-fa1d-4c53-b3fa-ffc9cd409040"
      },
      "outputs": [],
      "source": [
        "num_clients = 10\n",
        "\n",
        "# Create FedAvg strategy\n",
        "strategy = fl.server.strategy.FedAvg(\n",
        "    fraction_fit=0.9,  \n",
        "    fraction_evaluate=1,  \n",
        "    min_fit_clients=9,  \n",
        "    min_evaluate_clients=9,  \n",
        "    min_available_clients=int(\n",
        "        num_clients * 0.9\n",
        "    ),  \n",
        "    evaluate_metrics_aggregation_fn=weighted_average,\n",
        ")\n",
        "\n",
        "# Start simulation\n",
        "history = fl.simulation.start_simulation(\n",
        "    client_fn=client_fn_random,\n",
        "    num_clients=num_clients,\n",
        "    config=fl.server.ServerConfig(num_rounds=5),\n",
        "    strategy=strategy,\n",
        "    \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjQznojrgy46"
      },
      "source": [
        "Printamos os resultados do treinamento retornados no formato do objeto *history*. Ele tem dois atributos: um dicionário com a média da função de perda a cada *round* e um dicionário com os valores das métricas agregadas a cada *round*, calculadas pela função de agregação. \n",
        "\n",
        "Podemos observar a evolução destes dois valores a cada *round*. Para os 4 primeiros *rounds* tivemos uma melhora do valor da função de perda e da acurácia, mas no último *round* a função de perda diminuiu pouco e a acurácia se manteve estável.\n",
        "\n",
        "O número de *rounds* de treinamento é um valor muito importante para o aprendizado federado. Similar ao número de épocas de treinamento no aprendizado local, precisamos de *rounds* suficientes para que o modelo convirja com pesos adequados, mas temos que tomar cuidado para não causar o *overfitting* da rede, caso tenhamos muitos *rounds*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLw296UPIIIw",
        "outputId": "57ce75a3-9dba-4fbb-d9cb-d69846206936"
      },
      "outputs": [],
      "source": [
        "print(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbSgu-8-lDm5"
      },
      "source": [
        "A seguir plotaremos a acurácia por *round*. Observe que ela parece estar tendendo a aumentar a cada *round* e, **talvez**, se aumentássemos o numero de *rounds* teríamos uma acurácia ainda melhor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "K0OJfjkxW9NX",
        "outputId": "1d0fb1f3-fc89-4ea2-9694-a3ae26b6e3d0"
      },
      "outputs": [],
      "source": [
        "plt.plot(*zip(*history.metrics_distributed['accuracy']))\n",
        "plt.xlabel(\"Rounds\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R6RJStQTnCc"
      },
      "source": [
        "#### Atividades"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8LqayNw6xLV"
      },
      "source": [
        "**Atividade 1** \n",
        "\n",
        "Durante a configuração do treinamento federado definimos valores para muitos hyper-parâmetros. Um dos mais importantes é foi o número de *rounds*. Por isso nesta tarefa vocês deverão treinar a rede de maneira federada variando o número de rounds em diferentes valores. \n",
        "\n",
        " \n",
        "\n",
        "*   Os valores a serem usados para o número de *rounds* são 10, 15, 20;\n",
        "*   Para cada valor deve-se plotar um gráfico de linha que relaciona o número de *rounds* com a acurácia obtida naquele número de *rounds*.\n",
        "\n",
        "Além   disso deve-se comparar a desempenho do modelo federado com os diferentes valores de *rounds* e o  desempenho encontrado quando treinamos localmente a rede neural.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjhS4hFxPoBt"
      },
      "source": [
        "**Atividade 2** \n",
        "\n",
        "Fizemos o aprendizado federado mediado por uma simulação, mas em um cenário real teríamos processos clientes, representando os treinadores, sendo executados em várias máquinas independentes, conectadas a um processo servidor (agregador) pela rede ou pela Internet, por exemplo. \n",
        "\n",
        "Como tarefa para casa deve-se  realizar o aprendizado federado de uma maneira mais próxima da realidade, usando processos diferentes para rodar os clientes e o servidor. \n",
        "\n",
        "Para isso será necessário criar 2 programas python, o *client.py* e o *server.py*: \n",
        "\n",
        "\n",
        "*   O *client.py* deve implementar uma classe que herda a classe  *NumpyClient*, fornecida pela biblioteca flower, instanciar um objeto dessa classe e se conectar ao servidor;\n",
        "*   O *server.py* deve configurar um servidor de aprendizado federado usando um objeto *strategy* e um objeto *serverConfigserverConfig*. Em seguida deve-se iniciar o servidor para realizar o aprendizado com (no mínimo) 5 clientes.\n",
        "\n",
        "O *server.py* e o *client.py* devem ser executados em terminais próprios. Isto é, deve se usar um minimo de  6 terminais na solução dessa tarefa: um para o  servidor e um para cada cliente (5 no mínimo). \n",
        "\n",
        "Ao final, plotar a acurácia para 2,5, 10, 20 e 40 *rounds* como a quantidade de clientes definida por você (mínimo  5).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06-xKYJVd_t-"
      },
      "source": [
        "Dicas:\n",
        "\n",
        "\n",
        "\n",
        "*   Não é necessário utilizar várias máquinas para isso;\n",
        "\n",
        "*    Nesta atividade podemos emular uma rede por meio da interface de rede *localhost* (IP: 127.0.0.1) onde os processos treinadores e servidores podem ser executados em terminais Linux separados conectando-se pelo endereço IP 127.0.0.1;\n",
        "*   Como referência, usem a [documentação da biblioteca *flower*](https://flower.dev/docs/) na Seção QuickStart tutorials: tensorflow\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhW_2IZKVy15"
      },
      "source": [
        "**Atividade 3**\\\n",
        "**Obrigatória apenas para a pós-graduação** \n",
        "\n",
        "Usamos até agora uma rede neural convolucional para gerar o modelo treinado, tanto localmente como de maneira federada, mas o treinamento federado pode ser realizado (teoricamente) com qualquer classe de rede neural. \n",
        "\n",
        "Nesta atividade vocês deverão definir pelo menos 2 redes neurais não convolucionais para resolver o problema de classificação de dígitos com o *dataset mnist* e comparar os resultados do treinamento local e federado para essas redes.\n",
        "\n",
        "Dicas:\n",
        "\n",
        "\n",
        "*   O treinamento federado pode ser feito tanto por simulação, como este laboratório, ou com os programas *client.py* e *server.py* feitos na atividade anterior.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lab2-aprendizado-federado-bA2TOZBT-py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11 (main, Apr  7 2023, 07:31:31) [Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "b9ad06fa462f946d65d2c7ae9079e37b0e61a6b359fe3e56dff6748b374c0e88"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
